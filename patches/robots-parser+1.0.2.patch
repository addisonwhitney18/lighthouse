patch-package
--- a/node_modules/robots-parser/Robots.js
+++ b/node_modules/robots-parser/Robots.js
@@ -1,4 +1,4 @@
-var libUrl   = require('url');
+var URL   = (typeof self !== 'undefined' && self.URL) || require('url').URL;
 var punycode = require('punycode');
 
 /**
@@ -176,7 +176,7 @@ function isPathAllowed(path, rules) {
 
 
 function Robots(url, contents) {
-	this._url = libUrl.parse(url);
+	this._url = new URL(url);
 	this._url.port = this._url.port || 80;
 	this._url.hostname = punycode.toUnicode(this._url.hostname);
 
@@ -262,7 +262,7 @@ Robots.prototype.setPreferredHost = function (url) {
  * @return {boolean?}
  */
 Robots.prototype.isAllowed = function (url, ua) {
-	var parsedUrl = libUrl.parse(url);
+	var parsedUrl = new URL(url);
 	var userAgent = formatUserAgent(ua || '*');
 
 	parsedUrl.port = parsedUrl.port || 80;
@@ -277,7 +277,7 @@ Robots.prototype.isAllowed = function (url, ua) {
 
 	var rules = this._rules[userAgent] || this._rules['*'] || [];
 
-	return isPathAllowed(parsedUrl.path, rules);
+	return isPathAllowed(parsedUrl.pathname + parsedUrl.search, rules);
 };
 
 /**
